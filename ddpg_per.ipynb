{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import pybullet_envs\n",
    "from pathlib import Path\n",
    "from datetime import datetime \n",
    "# import DDPG\n",
    "# import original_buffer\n",
    "# import PER_buffer\n",
    "# import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from gym import spaces\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 'channels/channel_K6_AP32_2k_250x250.h5'\n",
    "channel_no = 1\n",
    "APs = 32\n",
    "UEs = 6\n",
    "# channel_K6_AP10_2k_20x20\n",
    "\n",
    "\n",
    "class wirelessCF(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, pu, nrx, gainspath=channels, channel_setup=channel_no, seed=0, eval = None):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    super(wirelessCF, self).__init__()\n",
    "    \n",
    "\n",
    "    # cf parameters\n",
    "    # self.channelGains = h5py.File(base_dir+str(gainspath), 'r')\n",
    "    self.channelGains = h5py.File(str(gainspath), 'r')\n",
    "    self.pu = np.power(10, pu/10 - 3)\n",
    "    self.B = 20000000\n",
    "    self.N0 = 1\n",
    "    self.T_c = 0.001\n",
    "    self.Ptcm = 0.2\n",
    "    self.Ptcl = 0.2\n",
    "    self.Pom = 0.825\n",
    "    self.pp = 0.2\n",
    "    self.K = UEs\n",
    "    self.tauc = 200\n",
    "    self.eff = 0.4 #amplifier efficiency\n",
    "    self.M = APs\n",
    "\n",
    "    self.Pft = 10\n",
    "    self.C_fh = 100000000\n",
    "    self.nu = 2\n",
    "    self.a = 0.88115\n",
    "    self.b = 0.88115\n",
    "    self.taup = self.K\n",
    "    self.tauf = 1 - (self.taup/self.tauc)\n",
    "    self.Nrx = nrx\n",
    "    self.R_fh = 2*self.K*self.nu*self.tauf*self.tauc/self.T_c\n",
    "    self.Pfix   = self.M*((self.Nrx*self.Ptcm)+self.Pom + self.Pft*self.R_fh/self.C_fh)/self.K\n",
    "    self.theta_max = 1\n",
    "    self.alpha = deque(maxlen=1)\n",
    "    self.beta = deque(maxlen=1)\n",
    "    self.ch_gain = deque(maxlen=1)\n",
    "    self.wi = [1/self.K for i in range(self.K)] # define equal weights which sum to 1\n",
    "    self.pi = deque(maxlen=1)\n",
    "    self.reward_timediff = deque([0,0],maxlen=2)\n",
    "    self.initialize_p()\n",
    "    self.channel_setup = channel_setup\n",
    "    self.initialize_matrix(channel_setup)\n",
    "    self.max_episode_step = 100\n",
    "    self._max_episode_steps = 100\n",
    "    # define action and obervation space:\n",
    "    self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                        shape=(self.K,), dtype=np.float32)\n",
    "    MIN_ACTION = 10e-7\n",
    "    self.action_space = spaces.Box(low=MIN_ACTION, high=1,\n",
    "                                  shape=(self.K,), dtype=np.float32)    \n",
    "\n",
    "    self.eval = eval\n",
    "\n",
    "    # history parameters\n",
    "    self.WSEE_history = []\n",
    "    self.sumSE_history = []\n",
    "    self.FPA_WSEE_history = []\n",
    "    self.FPA_sumSE_history = []\n",
    "    self.score_history = []\n",
    "    self.step_WSEE = []\n",
    "    self.step_sumSE = []\n",
    "    self.step_score = []\n",
    "    self.time_per_episode = []\n",
    "    self.curr_time = 0\n",
    "    self.end_time = 0\n",
    "\n",
    "    self.global_step = 0\n",
    "\n",
    "    # eval history\n",
    "    # self.eval_history = {}\n",
    "    # self.eval_history['WSEE'] = []\n",
    "    # self.eval_history['sumSE'] = []\n",
    "    # self.eval_history['FPA_WSEE'] = []\n",
    "    # self.eval_history['FPA_sumSE'] = []\n",
    "    # self.eval_history['score'] = []\n",
    "\n",
    "  def initialize_matrix(self, episode):\n",
    "        BETA = self.channelGains['channelGain'][episode].transpose()\n",
    "        # BETA = np.random.uniform(0,10,size=(1,self.K,self.M))\n",
    "        # h5f = h5py.File(str(dest)+'/rand_BETA.h5', 'w')\n",
    "        # h5f.create_dataset('channelGain', data=BETA)\n",
    "        # h5f.close()\n",
    "        # BETA = BETA[0].transpose()\n",
    "        print(BETA)\n",
    "        gamma_num = np.zeros((self.M,self.K))\n",
    "        gamma_den = np.zeros((self.M,self.K))\n",
    "\n",
    "        Gamma = np.zeros((self.M,self.K))\n",
    "        for m in range(self.M):\n",
    "            for k in range(self.K):\n",
    "                gamma_num[m][k] = self.taup*self.pp*np.power(BETA[m][k],2)                                  \n",
    "                gamma_den[m][k] = self.taup*self.pp*BETA[m][k]+1                    \n",
    "                Gamma[m][k] = gamma_num[m][k]/gamma_den[m][k]\n",
    "        self.ch_gain.append(Gamma)\n",
    "        alpha1 = np.zeros((self.K,))\n",
    "        for k in range(self.K):\n",
    "            #alpha1[k] = self.Nrx*self.pu*np.sum(Gamma[:][k])*self.pi[t][k]\n",
    "            alpha1[k] = self.pu*np.power(self.a*self.Nrx*np.sum(Gamma[:,k]),2)\n",
    "        self.alpha.append(alpha1)\n",
    "        beta1 = np.zeros((self.K,self.K))\n",
    "        for k in range(self.K):\n",
    "            for q in range(self.K):\n",
    "                beta1 [k][q] = self.a*self.a*self.pu*self.Nrx*(BETA[:,q].T@Gamma[:,k])         \n",
    "        self.beta.append(beta1)\n",
    "        return \n",
    "  \n",
    "  def initialize_p(self):\n",
    "      self.pi.append(np.random.uniform(low=0, high=self.theta_max, size=(self.K,)))\n",
    "      return\n",
    "    \n",
    "  def cal_alpha_p(self, i, t):\n",
    "      val = self.alpha[t][i]*self.pi[t][i]\n",
    "      return val\n",
    "  \n",
    "  def cal_beta_p(self, i, j, t):\n",
    "      val = self.beta[t][i][j]*self.pi[t][j] # channel from UE j to BS i\n",
    "      return val\n",
    "  \n",
    "  def sum_beta_p(self, i, t):\n",
    "      val = 0\n",
    "      for j in range(self.K):\n",
    "          val += self.cal_beta_p(i,j,t)  \n",
    "      return val \n",
    "\n",
    "  def cal_SINR(self,i,t):\n",
    "      val = self.cal_alpha_p(i,t)/(self.b*self.Nrx*np.sum(np.asarray(self.ch_gain)[t,:,i]) + \\\n",
    "                                              (self.b-self.a*self.a)*self.Nrx*self.Nrx*self.pu*self.pi[t][i]*np.linalg.norm(np.asarray(self.ch_gain)[t,:,i])**2 + \\\n",
    "                                              self.b/(self.a*self.a)*self.sum_beta_p(i, t))\n",
    "      return val\n",
    "\n",
    "  def cal_Ri(self,i, t):\n",
    "        val = np.log2(1+(self.cal_SINR(i,t)))\n",
    "        return val\n",
    "      \n",
    "  def cal_equal_p_Ri(self,i, t):\n",
    "      p = [self.theta_max for x in range(self.K)]\n",
    "      temp = self.pi.copy()\n",
    "      self.pi[t] = p\n",
    "      val = self.cal_Ri(i,t)\n",
    "      self.pi = temp\n",
    "      return val\n",
    "\n",
    "  def cal_EEi(self,i,t):\n",
    "      val = self.tauf*self.cal_Ri(i,t)/(self.pu*self.N0*self.pi[t][i]/self.eff + self.Pfix + self.Ptcl)\n",
    "      return val\n",
    "\n",
    "  def cal_total_WSEE(self,t):\n",
    "      val = 0\n",
    "      for x in range(self.K):\n",
    "          val += self.wi[x]*self.cal_EEi(x,t)\n",
    "      return val\n",
    "  \n",
    "  def cal_equal_p_WSEE(self, t):\n",
    "      p = [self.theta_max for x in range(self.K)]\n",
    "      temp = self.pi.copy()\n",
    "      self.pi[t] = p\n",
    "      val = self.cal_total_WSEE(t)\n",
    "      self.pi = temp\n",
    "      return val\n",
    "\n",
    "  def cal_SE_vec(self,t):\n",
    "      SEs = []\n",
    "      for u in range(self.K):\n",
    "        SE_u = self.tauf*self.cal_Ri(u,t)\n",
    "        SEs.append(SE_u)\n",
    "      return SEs\n",
    "\n",
    "  def cal_sum_SE(self,t):\n",
    "      SEs = self.cal_SE_vec(t)\n",
    "      sum_SE = sum([se for se in SEs])\n",
    "      return sum_SE\n",
    "\n",
    "  def cal_fpa_SE_vec(self,t):\n",
    "      SEs = []\n",
    "      for u in range(self.K):\n",
    "        SE_u = self.tauf*self.cal_equal_p_Ri(u,t)\n",
    "        SEs.append(SE_u)\n",
    "      return SEs\n",
    "\n",
    "  def cal_sum_fpa_SE(self,t):\n",
    "      SEs = self.cal_fpa_SE_vec(t)\n",
    "      sum_SE = sum([se for se in SEs])\n",
    "      return sum_SE\n",
    "\n",
    "  def cal_reward(self,t):\n",
    "      \"\"\"\n",
    "      SE optimization\n",
    "      \"\"\"\n",
    "      # ri = self.cal_sum_SE(t) \n",
    "      \"\"\"\n",
    "      WSEE optimization\n",
    "      \"\"\"\n",
    "      r = self.cal_total_WSEE(t) - self.cal_equal_p_WSEE(t)\n",
    "    #   ri = self.cal_total_WSEE(t)\n",
    "    #   self.reward_timediff.append(ri)\n",
    "    #   r = self.reward_timediff[1]-self.reward_timediff[0]\n",
    "      \"\"\"\n",
    "      SE+WSEE optimization\n",
    "      \"\"\"\n",
    "      # r = joint_weights*(self.cal_total_WSEE(t) - self.cal_equal_p_WSEE(t)) + (1-joint_weights)*(self.cal_sum_SE(t) - self.cal_sum_fpa_SE(t))\n",
    "      # r = (self.cal_total_WSEE(t) - self.cal_equal_p_WSEE(t)) + (self.cal_sum_SE(t) - self.cal_sum_fpa_SE(t))\n",
    "      return r\n",
    "      \n",
    "  def cal_state(self):\n",
    "      state = []\n",
    "      for t in range(1): #for current timestep only\n",
    "          for k in range(self.K):\n",
    "            state.append(self.cal_SINR(k,t)) #M\n",
    "          \n",
    "      # state = np.reshape(state, [1, self.observation_space.shape[0]])\n",
    "      normalized_states = state/np.max(state)\n",
    "      # return normalized_states\n",
    "      return -np.log10(state)\n",
    "\n",
    "  def initialize_state(self):\n",
    "      state = self.cal_state()\n",
    "      return state\n",
    "\n",
    "  def reset(self):\n",
    "      self.curr_time = time.time()\n",
    "      # self.initialize_matrix(self.channel_setup)\n",
    "      self.initialize_p()\n",
    "      self.episode_step = 0\n",
    "      state = self.initialize_state()\n",
    "      \n",
    "      return state\n",
    "\n",
    "          \n",
    "  def step(self, actions):\n",
    "      self.episode_step += 1\n",
    "      self.global_step += 1\n",
    "      # actions = actions[0] #theres an extra dim\n",
    "      self.pi.append(actions)\n",
    "      next_state = self.cal_state()\n",
    "      reward = self.cal_reward(0)\n",
    "      # next_state = np.reshape(next_state, [1, self.observation_space.shape[0]])\n",
    "      done = False\n",
    "      \n",
    "      self.step_WSEE.append(self.cal_total_WSEE(0))\n",
    "      self.step_sumSE.append(self.cal_sum_SE(0))\n",
    "      self.step_score.append(reward)\n",
    "      if self.episode_step >= self.max_episode_step:\n",
    "          \n",
    "          self.WSEE_history.append(np.mean(self.step_WSEE))\n",
    "          self.sumSE_history.append(np.mean(self.step_sumSE))\n",
    "          self.score_history.append(np.mean(self.step_score))\n",
    "          self.FPA_WSEE_history.append(self.cal_equal_p_WSEE(0))\n",
    "          self.FPA_sumSE_history.append(self.cal_sum_fpa_SE(0))\n",
    "\n",
    "\n",
    "          self.step_WSEE = []\n",
    "          self.step_sumSE = []\n",
    "          self.step_score = []\n",
    "\n",
    "          mode = \"Training\"\n",
    "          if self.eval:\n",
    "            mode = \"Evaluation\"\n",
    "          print(f'  {str(mode)}: Global Step: {self.global_step} || avg_score: {self.score_history[-1]} || WSEE: {self.WSEE_history[-1]} || FPA_WSEE: {self.cal_equal_p_WSEE(0)} || sumSE: {self.sumSE_history[-1]} || FPA_sumSE: {self.cal_sum_fpa_SE(0)}')\n",
    "\n",
    "          done = True\n",
    "          self.end_time = time.time()\n",
    "          self.time_per_episode.append(self.end_time - self.curr_time)\n",
    "          print('Time', self.time_per_episode[-1])\n",
    "          \n",
    "      info = {\n",
    "          'WSEE': self.cal_total_WSEE(0),\n",
    "          'FPA_WSEE': self.cal_equal_p_WSEE(0),\n",
    "          'sumSE': self.cal_sum_SE(0),\n",
    "          'FPA_sumSE': self.cal_sum_fpa_SE(0)\n",
    "      }\n",
    "      return next_state, reward, done, info\n",
    "\n",
    "  \n",
    "  def render(self, mode='human'):\n",
    "    NotImplementedError\n",
    "  def close (self):\n",
    "    NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import original_buffer\n",
    "import PER_buffer\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "fc1_dims = 500\n",
    "fc2_dims = 400\n",
    "fc3_dims = 300\n",
    "# fc4_dims = 30\n",
    "#Architecture from experimental details section of the DDPG \n",
    "#paper \"Continuous control with deep reinforcement learning\"\n",
    "#Lillicrap et. al. 2015\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, a_max, args):\n",
    "        super(Actor, self).__init__()\n",
    "        self.a_max = a_max\n",
    "        \n",
    "        self.l1 = nn.Linear(s_dim, args.fc1_dims)\n",
    "        self.l2 = nn.Linear(args.fc1_dims, args.fc2_dims)\n",
    "        self.l3 = nn.Linear(args.fc2_dims, args.fc3_dims)\n",
    "        # self.l4 = nn.Linear(fc3_dims, fc4_dims)\n",
    "        self.l5 = nn.Linear(args.fc3_dims, a_dim)\n",
    "  \n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.l1(s))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        # x = F.relu(self.l4(x))\n",
    "        x = torch.tanh(self.l5(x)) * self.a_max  \n",
    "        return x \n",
    "\n",
    "#Architecture from experimental details section of the DDPG\n",
    "#paper \"Continuous control with deep reinforcement learning\"\n",
    "#Lillicrap et. al. 2015\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, args):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(s_dim, args.fc1_dims)\n",
    "        self.l2 = nn.Linear(args.fc1_dims + a_dim, args.fc2_dims)\n",
    "        self.l3 = nn.Linear(args.fc2_dims, args.fc3_dims)\n",
    "        # self.l4 = nn.Linear(fc3_dims, fc4_dims)\n",
    "        self.l5 = nn.Linear(args.fc3_dims, 1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = F.relu(self.l1(s))\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        # x = F.relu(self.l4(x))\n",
    "        x = self.l5(x)\n",
    "        return x \n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, s_dim, a_dim, a_max, args):\n",
    "        #Create actor and actor target \n",
    "        self.actor = Actor(s_dim, a_dim, a_max, args).to(device)\n",
    "        self.actor_target = Actor(s_dim, a_dim, a_max, args).to(device)\n",
    "        #Initialize actor and actor target exactly the same\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        #Adam optimizer to train actor\n",
    "        #Learning rate specified in DDPG paper\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=args.lr_a)\n",
    "        \n",
    "        #Create critic and critic target\n",
    "        self.critic = Critic(s_dim, a_dim, args).to(device)\n",
    "        self.critic_target = Critic(s_dim, a_dim, args).to(device)\n",
    "        #Initialize critic and critic target exactly the same\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        #Adam optimizer to train critic\n",
    "        #L2 weight decay specified in DDPG paper\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=args.lr_c, weight_decay=1e-2)\n",
    "    \n",
    "    #Given a state, the actor returns a policy \n",
    "    def get_action(self, s):\n",
    "        s = torch.FloatTensor(s.reshape(1, -1)).to(device)\n",
    "        return self.actor(s).cpu().data.numpy().flatten()\n",
    "\n",
    "    #Update actor, critic and target networks with minibatch of experiences\n",
    "    def train(self, replay_buffer, prioritized, beta_value, epsilon, T, batch_size=64, gamma=0.99, tau=1e-4):\n",
    "\n",
    "        for i in range(T):\n",
    "            # Sample replay buffer\n",
    "            if prioritized: \n",
    "                #Prioritized experience replay\n",
    "                experience = replay_buffer.sample(batch_size, beta_value)\n",
    "                s, a, r, s_new, done, weights, batch_idxes = experience\n",
    "                #reshape data\n",
    "                r = r.reshape(-1, 1)\n",
    "                done = done.reshape(-1, 1)\n",
    "                #We do not use importance sampling weights\n",
    "                #Therefore importance sampling weights are all set to 1\n",
    "                #See Hyperparameter search in report \n",
    "                weights = np.ones_like(r)\n",
    "                #weights = weights.reshape(-1, 1)\n",
    "            else:\n",
    "                #Uniform experience replay\n",
    "                s, a, r, s_new, done = replay_buffer.sample(batch_size)\n",
    "                #importance sampling weights are all set to 1\n",
    "                weights, batch_idxes = np.ones_like(r), None\n",
    "\n",
    "            #Sqrt weights \n",
    "            #We do this since each weight will squared in MSE loss\n",
    "            weights = np.sqrt(weights)\n",
    "\n",
    "            #convert data to tensors\n",
    "            state = torch.FloatTensor(s).to(device)\n",
    "            action = torch.FloatTensor(a).to(device)\n",
    "            next_state = torch.FloatTensor(s_new).to(device)\n",
    "            done = torch.FloatTensor(1 - done).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "            weights = torch.FloatTensor(weights).to(device)\n",
    "   \n",
    "            #Compute the Q value estimate of the target network\n",
    "            Q_target = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            #Compute Y\n",
    "            Y = reward + (done * gamma * Q_target).detach()\n",
    "            #Compute Q value estimate of critic\n",
    "            Q = self.critic(state, action)\n",
    "            #Calculate TD errors\n",
    "            TD_errors = (Y - Q)\n",
    "            #Weight TD errors \n",
    "            weighted_TD_errors = torch.mul(TD_errors, weights)\n",
    "            #Create a zero tensor\n",
    "            zero_tensor = torch.zeros(weighted_TD_errors.shape)\n",
    "            #Compute critic loss, MSE of weighted TD_r\n",
    "            critic_loss = F.mse_loss(weighted_TD_errors,zero_tensor)\n",
    "\n",
    "            #Update critic by minimizing the loss\n",
    "            #https://pytorch.org/docs/stable/optim.html\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "            #Update the actor policy using the sampled policy gradient:\n",
    "            #https://pytorch.org/docs/stable/optim.html\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the target models\n",
    "            for critic_weights, critic__target_weights in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                critic__target_weights.data.copy_(tau * critic_weights.data + (1 - tau) * critic__target_weights.data)\n",
    "            for actor_weights, actor__target_weights in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                actor__target_weights.data.copy_(tau * actor_weights.data + (1 - tau) * actor__target_weights.data)\n",
    "    \n",
    "            #For prioritized exprience replay\n",
    "            #Update priorities of experiences with TD errors\n",
    "            if prioritized:\n",
    "                td_errors = TD_errors.detach().numpy()\n",
    "                new_priorities = np.abs(td_errors) + epsilon\n",
    "                replay_buffer.update_priorities(batch_idxes, new_priorities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer_org(object):\n",
    "    def __init__(self, capacity=1e6):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        #used to track which experience is to be removed next\n",
    "        self.current_pos = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        #If buffer is at capacity, remove the experience that has been in\n",
    "        #for the longest time and add the new experience in its place\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            self.buffer[int(self.current_pos)] = data\n",
    "            self.current_pos = (self.current_pos + 1) % self.capacity\n",
    "        #If buffer is not at capacity, just add experience\n",
    "        else:\n",
    "            self.buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        #get minibatch of indexes\n",
    "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        s, s_new, a, r, d = [], [], [], [], []\n",
    "        #copy experience data from buffer into minibatch\n",
    "        for index in indexes:\n",
    "            states, actions, rewards, new_states, done = self.buffer[index]\n",
    "            s.append(np.array(states, copy=False))\n",
    "            a.append(np.array(actions, copy=False))\n",
    "            r.append(np.array(rewards, copy=False))\n",
    "            s_new.append(np.array(new_states, copy=False))\n",
    "            d.append(np.array(done, copy=False))\n",
    "        #return minibatch of experience data\n",
    "        return np.array(s), np.array(a), np.array(r).reshape(-1, 1), np.array(s_new), np.array(d).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = utils.SumSegmentTree(it_capacity)\n",
    "        self._it_min = utils.MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super().add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = random.random() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient ( O(log segment size) )\n",
    "               `reduce` operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must form a mathematical group together with the set of\n",
    "            possible values for array elements (i.e. be associative)\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "class OrnsteinUlhenbeckActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.2, theta=0.2, dt=5e-2, x0=None):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset() # reset the noise\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta*(self.mu-self.x_prev)*self.dt + \\\n",
    "            self.sigma*np.sqrt(self.dt)*np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUlhenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_policy(policy):\n",
    "    episode_rewards = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        #reset environment\n",
    "        #get initial state\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        #Loop until end of episode\n",
    "        while not done:\n",
    "            #get action from policy\n",
    "            #In testing we do not use exploration noise\n",
    "            a = policy.get_action(np.array(s))\n",
    "            a = np.clip(a, env.action_space.low[0], env.action_space.high[0])\n",
    "            #take step in environment with action\n",
    "            #observe new state, reward and whether the episode is done\n",
    "            s_new, r, done, _ = env.step(a)\n",
    "            #add reward to episode reward\n",
    "            episode_rewards[i] += r\n",
    "            #update states\n",
    "            s = s_new\n",
    "    #calculate average episode reward over 10 tests\n",
    "    print(np.mean(episode_rewards))\n",
    "    return np.mean(episode_rewards)\n",
    "\n",
    "def main(args):\n",
    "    #Boolean to specify if we use prioritized experience replay\n",
    "    #False = original DDPG\n",
    "    #True = DDPG + PER\n",
    "    prioritized = True\n",
    "    POWER = args.power\n",
    "    channel_no = 1\n",
    "    #specificy PyBullet environment\n",
    "    # env_name = \"InvertedPendulumBulletEnv-v0\"\n",
    "    # env_name = \"Pendulum-v0\"\n",
    "    #env_name = \"HopperBulletEnv-v0\"\n",
    "    env_name = \"wirelessCF\"\n",
    "    #Set random seed number\n",
    "    seed = args.seed\n",
    "\n",
    "    #Output format\n",
    "    #True = training and testing results\n",
    "    #False = only testing results are printed\n",
    "    verbose = True\n",
    "\n",
    "\n",
    "    #Fixed Parameters\n",
    "    test_freq = 2000\n",
    "    train_steps = 100*args.n_episodes\n",
    "    NUM_EPISODES = args.n_episodes\n",
    "    Tag = 'WSEEonly'\n",
    "    #DDPG parameters\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    buffer_size = int(args.buffer_size)\n",
    "\n",
    "    #PER parameters\n",
    "    prioritized_replay_alpha=0.6\n",
    "    prioritized_replay_beta0=0.4\n",
    "    prioritized_replay_beta_iters=None\n",
    "    prioritized_replay_eps=1e-6\n",
    "    if prioritized:\n",
    "        file_name = \"Priority_DDPG_\"+env_name[:-12]+\"_\"+str(seed)\n",
    "    else:\n",
    "        file_name = \"DDPG_\"+env_name[:-12]+\"_\"+str(seed)\n",
    "\n",
    "    print('Running: '+file_name)\n",
    "\n",
    "    # env = gym.make(env_name)\n",
    "    env = wirelessCF(pu=POWER, nrx=2, seed=seed)\n",
    "    eval_env = wirelessCF(pu=POWER, nrx=2, seed=seed, eval=True)\n",
    "    # Set seeds\n",
    "    # env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # environment information\n",
    "    s_dim = env.observation_space.shape[0]\n",
    "    a_dim = env.action_space.shape[0]\n",
    "    a_max = float(env.action_space.high[0])\n",
    "\n",
    "    #Create DDPG policy\n",
    "    policy = DDPG(s_dim, a_dim, a_max, args)\n",
    "\n",
    "    #If we are not doing prioritized experience replay\n",
    "    #Then we use my implementation of the uniform replay buffer\n",
    "    if not prioritized:\n",
    "        replay_buffer = ReplayBuffer_org()\n",
    "        beta_schedule = None\n",
    "    #If we are doing prioritized experience replay\n",
    "    #Then we use the OpenAI implementation\n",
    "    else:\n",
    "        replay_buffer = PrioritizedReplayBuffer(buffer_size, prioritized_replay_alpha)\n",
    "        if prioritized_replay_beta_iters is None:\n",
    "            prioritized_replay_beta_iters = train_steps\n",
    "        #Create annealing schedule\n",
    "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters, initial_p=prioritized_replay_beta0, final_p=1.0)\n",
    "\n",
    "    #test policy (at this point policy is just random)\n",
    "    # test_scores = [test_policy(policy)]\n",
    "    #Save data file (used for plotting)\n",
    "    # np.save(\"data/%s\" % (file_name), test_scores)\n",
    "\n",
    "    OUnoise = OrnsteinUlhenbeckActionNoise(mu=np.zeros(env.action_space.shape[0]))\n",
    "\n",
    "    total_time = 0\n",
    "    test_time = 0\n",
    "    episode = 0\n",
    "    done = True\n",
    "\n",
    "    t_0 = time.time()\n",
    "\n",
    "    while total_time < train_steps:\n",
    "\n",
    "        if done:\n",
    "\n",
    "            if total_time != 0:\n",
    "                if verbose:\n",
    "                    print(\"Total Time Steps: \"+str(total_time)+ \" Episode Reward: \"+str(episode_r)+\" Runtime: \"+str(int(time.time() - t_0)))\n",
    "\n",
    "                #set beta value used for importance sampling weights\n",
    "                beta_value = 0\n",
    "                if prioritized:\n",
    "                    beta_value = beta_schedule.value(total_time)\n",
    "\n",
    "                #train DDPG\n",
    "                policy.train(replay_buffer, prioritized, beta_value, prioritized_replay_eps, episode_t, args.batch_size, args.gamma, args.tau)\n",
    "\n",
    "            #Check if we need to need to test DDPG\n",
    "            # if test_time >= test_freq:\n",
    "            #     test_time %= test_freq\n",
    "            #     test_scores.append(test_policy(policy))\n",
    "            #     np.save(\"data/%s\" % (file_name), test_scores)\n",
    "\n",
    "            # evaluation\n",
    "            eval_s = eval_env.reset()\n",
    "            eval_done = False\n",
    "            while not eval_done:\n",
    "                eval_a = policy.get_action(np.array(eval_s))\n",
    "                eval_a = np.clip(eval_a, eval_env.action_space.low[0], eval_env.action_space.high[0])\n",
    "                eval_s, eval_r, eval_done, _ = eval_env.step(eval_a)\n",
    "                \n",
    "\n",
    "            #reset environment\n",
    "            #get intial state\n",
    "            #reset episode statistics\n",
    "            s = env.reset()\n",
    "            OUnoise.reset()\n",
    "            done = False\n",
    "            episode_r = 0\n",
    "            episode_t = 0\n",
    "            episode += 1\n",
    "\n",
    "\n",
    "        #Given current state, get action\n",
    "        a = policy.get_action(np.array(s))\n",
    "        #Apply exploration noise to action\n",
    "        # a = (a + np.random.normal(0, 0.1, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "        a = a + OUnoise()\n",
    "        a = np.clip(a, env.action_space.low, env.action_space.high)\n",
    "        #Using action, take step in environment, observe new state, reward and episode status\n",
    "        s_new, r, done, _ = env.step(a)\n",
    "        done_bool = 0 if episode_t + 1 == env._max_episode_steps else float(done)\n",
    "        episode_r += r\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        if not prioritized:\n",
    "            replay_buffer.add((s, a, r, s_new, done_bool))\n",
    "        else:\n",
    "            replay_buffer.add(s, a, r, s_new, done_bool)\n",
    "\n",
    "        #update state and episode statistics\n",
    "        s = s_new\n",
    "        episode_t += 1\n",
    "        total_time += 1\n",
    "        test_time += 1\n",
    "\n",
    "    #Final policy test\n",
    "    # test_scores.append(test_policy(policy))\n",
    "    # #Save data file (used for plotting)\n",
    "    # np.save(\"data/%s\" % (file_name), test_scores)\n",
    "    # print(test_scores)\n",
    "    config = 'lr_a'+str(args.lr_a)+'lr_c'+str(args.lr_c)+'tau_'+str(args.tau)+'gamma_'+str(args.gamma)+'_batch_'+str(args.batch_size)+'_layers'+str(args.fc1_dims)+'_'+str(args.fc2_dims)+'_'+str(args.fc3_dims)\n",
    "    path = Path('testlogs_'+'power_'+str(POWER)+'_channel_'+str(channel_no)+'_'+str(Tag)+'_config_'+str(config))\n",
    "    dest = Path(str(path))\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    # dest = 'testlogs'\n",
    "    import pandas as pd\n",
    "    ########################## Random Scheme ##################################\n",
    "    temp = env.pi\n",
    "    random_WSEE = []\n",
    "    # random_SEs = []\n",
    "    random_sum_SE = []\n",
    "    # random_constraint_value = []\n",
    "    for i in range(NUM_EPISODES):\n",
    "        action = np.random.uniform(0,1,(env.action_space.shape[0]))\n",
    "        env.pi.append(action)\n",
    "        next_state = env.cal_state()\n",
    "        reward = env.cal_reward(0)\n",
    "        # next_state = np.reshape(next_state, [1, self.observation_space.shape[0]])\n",
    "        done = False\n",
    "        # random_SEs.append(env.SEs.tolist())\n",
    "        random_sum_SE.append(env.cal_sum_SE(0))\n",
    "        random_WSEE.append(env.cal_total_WSEE(0))\n",
    "        # print(random_sum_SE, random_WSEE)\n",
    "        # print()\n",
    "    \n",
    "    env.pi = temp\n",
    "    powerdbm = POWER\n",
    "\n",
    "    rand_df_wsee = pd.DataFrame(data=random_WSEE, columns=['WSEE'])\n",
    "    rand_df_wsee.to_csv(str(dest)+'/'+str(Tag)+'rand_WSEE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    # rand_data_se = pd.DataFrame(data=random_SEs,columns=['SE'+str(i) for i in range(1,env.K+1)])\n",
    "    # rand_data_se.to_csv(str(dest)+'/'+str(Tag)+'rand_SEs'+str(powerdbm)+'.csv')\n",
    "\n",
    "    rand_df_sumSE = pd.DataFrame(data=random_sum_SE, columns=['sum_SE'])\n",
    "    rand_df_sumSE.to_csv(str(dest)+'/'+str(Tag)+'rand_sum_SE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    ###########################################################################\n",
    "    episode = [i for i in range(NUM_EPISODES)]\n",
    "    eval_episode = [i for i in range(len(eval_env.WSEE_history))]\n",
    "    powerdbm = POWER\n",
    "\n",
    "    df_wsee = pd.DataFrame(data=env.WSEE_history, columns=['WSEE'])\n",
    "    df_wsee.to_csv(str(dest)+'/'+str(Tag)+'WSEE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_eval_wsee = pd.DataFrame(data=eval_env.WSEE_history, columns=['WSEE'])\n",
    "    df_eval_wsee.to_csv(str(dest)+'/'+str(Tag)+'eval_WSEE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_fpawsee = pd.DataFrame(data=env.FPA_WSEE_history, columns=['WSEE'])\n",
    "    df_fpawsee.to_csv(str(dest)+'/'+str(Tag)+'FPA_WSEE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_reward = pd.DataFrame(data=env.score_history, columns=['reward'])\n",
    "    df_reward.to_csv(str(dest)+'/'+str(Tag)+'reward'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_eval_reward = pd.DataFrame(data=eval_env.score_history, columns=['reward'])\n",
    "    df_eval_reward.to_csv(str(dest)+'/'+str(Tag)+'eval_reward'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_sumSE = pd.DataFrame(data=env.sumSE_history, columns=['sum_SE'])\n",
    "    df_sumSE.to_csv(str(dest)+'/'+str(Tag)+'sum_SE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_eval_sumSE = pd.DataFrame(data=eval_env.sumSE_history, columns=['sum_SE'])\n",
    "    df_eval_sumSE.to_csv(str(dest)+'/'+str(Tag)+'eval_sum_SE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_fpasumSE = pd.DataFrame(data=env.FPA_sumSE_history, columns=['sum_SE'])\n",
    "    df_fpasumSE.to_csv(str(dest)+'/'+str(Tag)+'FPA_sum_SE'+str(powerdbm)+'.csv')\n",
    "\n",
    "    df_time_per_episode = pd.DataFrame(data=env.time_per_episode, columns=['time'])\n",
    "    df_time_per_episode.to_csv(str(dest)+'/'+str(Tag)+'time_per_episode'+str(powerdbm)+'.csv')\n",
    "    \n",
    "    def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "        alpha = 2 /(window + 1.0)\n",
    "        alpha_rev = 1-alpha\n",
    "        n = data.shape[0]\n",
    "\n",
    "        pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "        scale_arr = 1/pows[:-1]\n",
    "        offset = data[0]*pows[1:]\n",
    "        pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "        mult = data*pw0*scale_arr\n",
    "        cumsums = mult.cumsum()\n",
    "        out = offset + cumsums*scale_arr[::-1]\n",
    "        return out\n",
    "\n",
    "    smooth_window = 20\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    reward = numpy_ewma_vectorized_v2(np.array(env.score_history),smooth_window)\n",
    "    episode = np.array(episode)\n",
    "\n",
    "    plt.plot(episode, reward,'-' ,label='Reward')\n",
    "    plt.title('Training reward over multiple runs')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.legend()\n",
    "    plt.savefig(dest/r'reward.jpg', dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    out_WSEE = numpy_ewma_vectorized_v2(np.array(env.WSEE_history),smooth_window)\n",
    "    FPA_WSEE = numpy_ewma_vectorized_v2(np.array(env.FPA_WSEE_history),smooth_window)\n",
    "    rand_WSEE = numpy_ewma_vectorized_v2(np.array(random_WSEE),smooth_window)\n",
    "\n",
    "    plt.plot(episode, out_WSEE,'-' ,label='DDPG '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, FPA_WSEE,'-' ,label='FPA '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, rand_WSEE,'-' ,label='Random Scheme '+str(POWER)+'dBm')\n",
    "    plt.title('WSEE: DDPG vs FPA vs Random Scheme')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('WSEE')\n",
    "    plt.legend()\n",
    "    plt.savefig(dest/r'WSEE.jpg', dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    out_eval_WSEE = numpy_ewma_vectorized_v2(np.array(eval_env.WSEE_history),smooth_window)\n",
    "    FPA_eval_WSEE = numpy_ewma_vectorized_v2(np.array(eval_env.FPA_WSEE_history),smooth_window)\n",
    "    rand_WSEE = numpy_ewma_vectorized_v2(np.array(random_WSEE),smooth_window)\n",
    "\n",
    "    plt.plot(eval_episode, out_eval_WSEE,'-' ,label='DDPG '+str(POWER)+'dBm')\n",
    "    plt.plot(eval_episode, FPA_eval_WSEE,'-' ,label='FPA '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, rand_WSEE,'-' ,label='Random Scheme '+str(POWER)+'dBm')\n",
    "    plt.title(' WSEE Evalutation: DDPG vs FPA vs Random Scheme')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('WSEE')\n",
    "    plt.legend()\n",
    "    plt.savefig(dest/r'eval_WSEE.jpg', dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    out_sum_SE = numpy_ewma_vectorized_v2(np.array(env.sumSE_history),smooth_window)\n",
    "    FPA_sum_SE = numpy_ewma_vectorized_v2(np.array(env.FPA_sumSE_history),smooth_window)\n",
    "    rand_sum_SE = numpy_ewma_vectorized_v2(np.array(random_sum_SE),smooth_window)\n",
    "\n",
    "    plt.plot(episode, out_sum_SE,'-' ,label='DDPG '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, FPA_sum_SE,'-' ,label='FPA '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, rand_sum_SE,'-' ,label='Random Scheme '+str(POWER)+'dBm')\n",
    "    plt.title('sum SE: DDPG vs FPA vs Random Scheme')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('sum SE')\n",
    "    plt.legend()\n",
    "    plt.savefig(dest/r'sum_SE.jpg', dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    out_eval_sum_SE = numpy_ewma_vectorized_v2(np.array(eval_env.sumSE_history),smooth_window)\n",
    "    FPA_eval_sum_SE = numpy_ewma_vectorized_v2(np.array(eval_env.FPA_sumSE_history),smooth_window)\n",
    "    rand_sum_SE = numpy_ewma_vectorized_v2(np.array(random_sum_SE),smooth_window)\n",
    "\n",
    "    plt.plot(eval_episode, out_eval_sum_SE,'-' ,label='DDPG '+str(POWER)+'dBm')\n",
    "    plt.plot(eval_episode, FPA_eval_sum_SE,'-' ,label='FPA '+str(POWER)+'dBm')\n",
    "    plt.plot(episode, rand_sum_SE,'-' ,label='Random Scheme '+str(POWER)+'dBm')\n",
    "    plt.title('sum SE Evaluation: DDPG vs FPA vs Random Scheme')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('sum SE')\n",
    "    plt.legend()\n",
    "    plt.savefig(dest/r'eval_sum_SE.jpg', dpi=300)\n",
    "    # plt.show()\n",
    "    print('Mean time per episode: ', df_time_per_episode.stack().mean())\n",
    "    print('Std of time: ', df_time_per_episode.stack().std())\n",
    "    print('Max: ', df_time_per_episode.stack().max())\n",
    "    print('Min: ', df_time_per_episode.stack().min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "import argparse\n",
    "# if __name__ == '__main__':\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Commandline utility for training RL Models'\n",
    ")\n",
    "parser.add_argument('-seed', type=int, default=143, help='Seed value')\n",
    "parser.add_argument('-n_episodes', type=int, default=2500, help='Number of episodes to run for')\n",
    "parser.add_argument('-lr_a', type=float, default=1e-7, help='Learning rate for actor')\n",
    "parser.add_argument('-lr_c', type=float, default=1e-5, help='Learning rate for critic')\n",
    "parser.add_argument('-tau', type=float, default=1e-5, help='Polyak averaging factor')\n",
    "parser.add_argument('-batch_size', type=int, default=40, help='Batch size for replay memory sampling')\n",
    "parser.add_argument('-gamma', type=float, default=0.6, help='Discount factor')\n",
    "parser.add_argument('-buffer_size', type=int, default=1e6, help='Maximum replay memory size')\n",
    "parser.add_argument('-fc1_dims', type=int, default=500, help='Hidden Layer 1 dimensions')\n",
    "parser.add_argument('-fc2_dims', type=int, default=400, help='Hidden Layer 2 dimensions')\n",
    "parser.add_argument('-fc3_dims', type=int, default=300, help='Hidden Layer 3 dimensions')\n",
    "parser.add_argument('-power', type=int, default=30, help='Power in dBm')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "# args.dims = [args.dims]\n",
    "total_time_start = time.time()\n",
    "main(args)\n",
    "total_time_end = time.time()\n",
    "print('Total time taken: ', total_time_end - total_time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f6e60986ef3157b4d0e3f187c33164a93b80fa00f892d79e07b8be18aada22"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('env_bilevel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
